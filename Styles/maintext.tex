\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage{graphicx} 
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{natbib}
\usepackage{tabularx}

\title{Enhancing Reasoning to Adapt Large Language Models for Domain-Specific Applications}

\author{%
  Bo Wen\\
  IBM Watson Research Center, \\
  Yorktown Heights, NY, USA \\
  \texttt{bwen@us.ibm.com} \\
  \And
  Xin Zhang\\
  IBM Watson Research Center, \\
  Yorktown Heights, NY, USA \\
  \texttt{xzhang@us.ibm.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This paper explores the potential of large language models (LLMs) as an adaptive ``layout design copilot'' in semiconductor and integrated circuit (IC) design. We evaluate the baseline performance of state-of-the-art LLMs on a set of 25 layout design tasks, revealing common mistakes and limitations. We explore spatial reasoning and knowledge application capabilities of LLMs through the Via Connection test case, demonstrating the challenges of adapting general-purpose models to specialized domains. We introduce a novel Neuro-inspired LLM Reasoning Network architecture, called SOLOMON, which significantly improves performance on these tasks through In-Context Learning and Prompt Tuning. The paper discusses the challenges of using LLMs in layout design and proposes future research directions for developing more adaptive AI systems.
\end{abstract}

\section{Introduction}
The rapid advancements in large language models (LLMs) have revolutionized various aspects of artificial intelligence, enabling them to understand and generate human-like text with remarkable proficiency. However, adapting these general-purpose models to domain-specific tasks remains a significant challenge. In this paper, we introduce SOLOMON, a Neuro-inspired LLM Reasoning Network Architecture that leverages Prompt Tuning and In-Context Learning techniques, and demonstrate how SOLOMON can effectively adapt from its original design purpose in medical applications to a new domain: semiconductor layout design. Section \ref{sec:architecture} presents the SOLOMON architecture and highlights its design principles that contribute to enhanced adaptability.

To provide context for our experiment, we first examine how a designer might attempt to use ChatGPT (with GPT-4o mode) for a via connection design task in section \ref{sec:problem}. This exploration reveals a critical limitation: while LLMs can accurately recite textbook definitions of domain-specific concepts, they struggle to extract and apply expert knowledge to solve practical tasks. Human needs to translate high-level concepts into specific geometric requirements, which the LLM can then use to generate code for drawing shapes. This highlights the key challenge in adapting LLMs for domain-specific applications: their limited reasoning capabilities.

In section \ref{sec:evaluation}, we developed a set of 25 tasks ranging from basic geometric shapes to complex semiconductor structures, to evaluate our SOLOMON architecture against five different LLMs. These tasks assess spatial reasoning capabilities and adaptability across various complexity levels. Through these experiments, we demonstrate SOLOMON's superior performance compared to standalone LLMs, and reaching the level of state-of-the-art reasoning models like O1-preview. 

Our findings emphasize the crucial role of reasoning in enhancing LLMs' adaptability to diverse domain applications. This study contributes to ongoing research in adaptive foundation models, providing insights into how to improve reasoning capabilities with inspiration from neuroscience.

\section{Neuro-inspired LLM Reasoning Network Architecture}
\label{sec:architecture}
SOLOMON's architecture is inspired by two neuro-inspired theories: Brain-like AGI \cite{Byrnes2022} and the Free Energy Principle (FEP) theory \cite{Parr2022}. The former inspired us to use a pool of thoughts from multiple LLMs to extract the best reasoning plan. From the latter, we applied the FEP's claim that human attention focuses on minimizing the differences between goals and perceptions to select relevant information and avoid common pitfalls. The key components of SOLOMON are:

\paragraph{Thought Generators:} A diverse pool of LLMs generating thoughts for the target task. This component forms an efficient parallel search engine through the Tree-of-Thoughts \cite{yao2023treethoughtsdeliberateproblem, zhang2024cumulativereasoninglargelanguage, Besta_2024, besta2024demystifyingchainstreesgraphs} and functions as an adaptive RAG system for the Thought Assessor. By pooling thoughts from multiple LLMs with distinct knowledge bases and reasoning abilities, it provides a more flexible and effective source for sampling diverse ideas compared to common embedding-based RAG. This approach also mitigates biases inherent in single LLM knowledge bases. Noted that the individual LLMs in the Thought Generators can be further enhanced with proprietary knowledge through classic RAG techniques.

\paragraph{Thought Assessor:} An LLM-based system that analyzes the proposed ``Thoughts'' to generate a refined output. It conducts in-context learning on the Thought Generators' output and follows the Free Energy Principle for goal-oriented assessments on consensus and differences. This approach enhances the LLM-as-a-Judge method \cite{zheng2023judgingllmasajudgemtbenchchatbot, lin2023llmevalunifiedmultidimensionalautomatic}, enabling self-reflection \cite{ji2023mitigatinghallucinationlargelanguage} and guarding against hallucinations \cite{guerreiro2023lookingneedlehaystackcomprehensive}, thus improving AI safety and reliability.

\paragraph{Steering Subsystem:} A human-operated component that controls the attention focus for the Thought Generators and Thought Assessor. It uses Prompt Tuning to modify the goals of other components, enabling swift adaptation to different domain requirements through goal-directed exploration. This enhances the system's versatility across various applications by simply adjusting the attention focus.

This architecture offers significant advantages over traditional fine-tuning approaches, eliminating the need for recurrent fine-tuning when upgrading underlying LLMs or updating domain-specific knowledge. Basing on Prompt Engineering techniques, SOLOMON enables a more flexible and widely applicable system, capable of addressing diverse specialized contexts.

\section{Problem: Spatial Reasoning and Domain Knowledge Application}
\label{sec:problem}
Layout design in semiconductor processes requires not only generating correct basic geometric shapes but also spatial reasoning to create proper ``layouts'' that meet specific requirements. Via connections, which create electrical pathways between different chip layers, exemplify this challenge. While seemingly simple—typically consisting of circular vias and rectangular metal connections—they demand precise positioning and sizing to ensure no short or open circuits and other functionality issues.

We conducted a series of tests by providing a sketch(image) together with different text prompts to ChatGPT (GPT-4o). The sketch are color-coded to represent different layers (e.g., yellow for via, blue for metal, red for pad) to help LLM understand the spatial relationships. Figure \ref{fig:via_experiment} illustrates the sketch inputs and corresponding LLM-generated outputs for each test case.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{Figure1_v5.png}
\caption{Sketch input and LLM-generated outputs for the via connection experiment. The sketch depicts a desired layout with two vias connected by a metal layer and circular pads on top. The outputs show the progression of the LLM's understanding and refinement of the layout based on iterative feedback and context provided by the user.}
\label{fig:via_experiment}
\end{figure}
In \textbf{Test 1}, we provided a basic sketch (Figure ~\ref{fig:via_experiment}(a)) with simple instructions. The LLM generated code, but the output had issues, including insufficient metal connection width. \textbf{Test 2} involved refining the description, but the output incorrectly used square vias and pads, and failed to properly cover the vias with metal. In \textbf{Test 3}, we provided a more detailed description. After seven iterations of feedback, the LLM finally produced the correct layout (Figure ~\ref{fig:via_experiment}(d)). See Appendix \ref{appendix:via_connection} for detailed prompts and the iterative process.

\textbf{Test 4} reversed the process by asking the LLM to create a detailed prompt based on the correct layout from \textbf{Test 3} (Appendix \ref{appendix:via_connection}). The LLM described each component's size and location in detail but hallucinated an additional requirement: \textit{a 50-unit space between the vias and the edges of the metal connection}. This would result in the layout shown in the dotted rectangle in Figure~\ref{fig:via_experiment}(e), where the metal extends beyond the contact pad. Interestingly, when given this ``wrong'' prompt, GPT-4o ignored the added specification and produced a layout matching the original design, with the metal not extending beyond the pad. Code inspection revealed that the LLM used another requirement, \textit{Leave a margin of 10 units between the edge of the metal and the pads}, to calculate the metal edge position in both x and y directions, although this was intended only for the y-direction margin. Using this version of prompt in the baseline evaluations (Section 1), o1-preview and Llama-3.1-405B each produced the ``non-extending'' version in one out of 5 runs, indicating some ambiguity in the specification. Generally, providing specific numerical values for size and location of geometric shapes in the prompt proves more robust than simple instructions like ``draw me a via''.

To further test our hypothesis, we conducted \textbf{Tests 5} and \textbf{6}, removing numerical values from the prompt and incorporating domain-specific context (e.g., 3D packaging and Through-Silicon Vias). This approach, however, degraded LLM performance, revealing a critical limitation: while LLMs possess textbook knowledge of semiconductor concepts, they struggle to translate this into practical design requirements. For instance, LLMs failed to apply common engineering knowledge, such as using wider metal layers to connect vias or leaving margin space between components in different layers to account for layermisalignment.

This finding underscores the importance of improving LLMs' reasoning capability of applying domain knowledge in problem-solving, rather than simply increasing model size to remember more knowledge, to enhance the adaptability of LLM-based AI systems.

\section{SOLOMON Performance and Comparison}
\label{sec:evaluation}
To evaluate SOLOMON's effectiveness in enhancing spatial reasoning for semiconductor layout design, we conducted experiments using a dataset of 25 layout design tasks. These tasks were categorized into four groups: Basic Shapes 1 and 2 included simple geometric shapes such as circles, polygons and text, which serve as the building blocks for more complex layouts. The Advanced Shapes category involved more intricate designs, such as serpentine and spirals, to test the models' ability to handle complex geometries. Finally, the Complex Structures category included tasks that required the composition of multiple shapes to form functional layouts, such as a Dense Layer Diode (DLD) chip, MicrofluidicChip, and the ViaConnection test case. These tasks were designed to benchmark the AI systems' capability in generating layouts that are representative of real-world semiconductor design needs.

We provided task requirements with a system prompt asking the LLMs to use Chain-of-Thought to analyze the task and write Python code to create a GDSII output. The evaluation process involved running the generated code to produce GDSII files, which were then converted to PNG images. Human evaluators categorized the output into five categories: correct, scaling error, partially correct, shape error, and runtime error. Five LLMs (GPT-4o, Claude-3.5-Sonnet, Llama-3.1-70B, Llama-3.1-405B, and o1-preview) were used for the baseline experiment, with each task run 5 times per model. (See Appendix \ref{appendix:task_prompts_and_performance} for detailed task prompts and outputs.)

To evaluate SOLOMON, we utilized 20 thoughts generated by GPT-4o, Claude, and two Llama-3.1 models from the baseline experiment. We created four SOLOMON instances, each using one of these LLMs as a Thought Assessor, excluding o1-preview which served as our benchmark for state-of-the-art reasoning performance.

Figure \ref{fig:performance_comparison} presents a performance comparison between the SOLOMON instances, their baseline counterparts, and the o1-preview model.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{output.png}
  \caption{Performance comparison between SOLOMON instances, their baseline counterpart single LLMs, and o1-preview across different layout design task categories. Lighter colored bars represent baseline performance of individual LLMs, while darker bars show the performance of corresponding SOLOMON instances. O1-preview results serve as a benchmark for state-of-the-art reasoning performance.}
  \label{fig:performance_comparison}
\end{figure}

The results demonstrate that the SOLOMON architecture significantly improves the performance of all four LLMs compared to their baseline. The most notable improvements are observed in the reduction of runtime errors, which can be attributed to the Thought Assessor seeing the error log of previous generated code and knowing what to avoid. This aligns with the design principle of the hierarchical, self-reflection mechanism, which aims to mitigate individual LLM's hallucination and blind-spot.

The second most notable problem in baseline is scaling errors. We intentionally requested basic shapes to be drawn in millimeters to test whether LLMs could catch this trap (as the default unit in the gdspy library is micrometers). Besides that sometimes LLMs failed to pay attention to this trap and produced incorrect results, another big contribution of errors comes from LLMs inherent bias: even if they noticed the unit request, they would hallucinate the default unit of gdspy library to something incorrect: Llama models tended to assume millimeters, Claude models sometimes defaulted to nanometers, while GPT-4o occasionally interpreted the default unit as meters. This especially problemtic for Llama-3 models, who sometimes even insisted that the user had made a mistake by requesting millimeters, and proceeded to draw in micrometers instead, justifying this choice with comments like ``not mm, as the GDSII format is in micrometers''. This type of ``arrogant'' behavior and misalignment with human instructions on simple tasks could be harmful for deploying LLMs as fully autonomous AI agents. A recent Nature paper \cite{ZhouNature2024} has also discussed similar observations. 

With the SOLOMON architecture, even when Llama-3 models acting as the Thought Assessor, because now they can see alternative perspectives from different models, they are less ``stubborn'' and more likely to produce correct results. Similarly, SOLOMON instances demonstrate better performance in handling shape errors and partial correctness issues, as the Thought Assessor can detect and rectify errors stemming from arithmetic miscalculations or incorrect relative positioning of shapes.

Comparing SOLOMON instances with o1-preview, we find that SOLOMON achieves comparable or superior results. All SOLOMON instances outperformed o1-preview in Basic Shape 1 categories, with the Claude-3.5-Sonnet-based SOLOMON surpassing o1-preview in 3 categories overall.

Analysis of SOLOMON errors reveals that performance depends heavily on the quality and consensus of initial thoughts. Tasks with ambiguous requirements often leads to significant disagreement among initial thoughts, leading to confusion of Thought Assessor and degraded performance. Additionally, insufficient information linking images to corresponding code and error logs sometimes resulted in misinterpretations. These areas present opportunities for future improvements to the SOLOMON architecture.

\section{Conclusion and Future Work}
The introduction of the SOLOMON architecture significantly improved performance, particularly in reducing runtime errors and enhancing spatial reasoning capabilities. However, challenges remain in translating domain knowledge into practical design requirements.

Future research directions include:
(1) Creating more comprehensive benchmark datasets for LLM capabilities in layout design tasks.
(2) Linking the pngs with cooresponding code and error log in the thoughts, to evaluate further improvement.
(3) Investigate the optimal number and combination of input thoughts to the SOLOMON system.
(4) Invesitgate the performance of SOLOMON architecture when the initial thoughts are not of high quality. How to conduct goal-oriented iterative learning to improve the quality of initial thoughts with feedback loop.
(5) Apply SOLOMON architecture on more domain specific tasks, such as power grid design, and mechanical design.
(6) We didn't invesitgate the problem of extracting domain specific knowledge from the LLM in this paper. But by stacking multiple SOLOMON layers, we can form a hierarchical reasoning model, with the ability of recalling and reasoning domain knowledge for solving the task on hand.


(2) Exploring more configurations of SOLOMON architecture to understand the interaction between quality of initial thoughts vs assessment by ensemble, i.e., system 1 vs system 2 contributions in reasoning tasks.
(3) Exploring stacking layers of SOLOMON architecture to form a hierarchical reasoning model, for recalling and reasoning domain knowledge for solving the task on hand.
(4) Exploring the adaptability of SOLOMON architecture across different domains.

In conclusion, the experimental results validate the effectiveness of the SOLOMON architecture in enhancing the spatial reasoning capabilities and adaptability of LLMs for semiconductor layout design tasks. The significant performance improvements, particularly in reducing runtime errors and enhancing spatial reasoning capabilities, highlight the potential of this neuro-inspired approach in advancing the field of adaptive foundation models. The comparable performance of SOLOMON to the state-of-the-art o1-preview model further demonstrates its capability in tackling complex spatial reasoning tasks. As the SOLOMON architecture continues to evolve and incorporate advancements in LLMs and domain-specific knowledge, it holds promise for revolutionizing the field of semiconductor design and other domains that require robust spatial reasoning abilities.

While LLMs show promise as layout design copilots, further advancements in reasoning capabilities and domain knowledge application are necessary for their effective integration into semiconductor design processes and other domain specific tasks.

\textbf{Acknowledgment:} We thank Kuan Yu Hsieh for her valuable contribution in creating the dataset of 25 tasks with ground truth and her exploration work on the via connection test cases.

\bibliographystyle{plainnat}
\bibliography{bibliography}

% Add the NeurIPS Paper Checklist here, before any supplementary material
\newpage
\section*{Checklist}

\input{checklist.tex}

\newpage
\appendix

\section{Appendix}
\subsection{Task Prompts and Baseline LLM Performance}
\label{appendix:task_prompts_and_performance}

This section presents the 25 tasks used in our evaluation, along with the performance of various LLMs and the SOLOMON system. For each task, we provide the prompt, the ground truth image, and a table showing the outputs from different LLMs across 5 runs, as well as the SOLOMON result.

The system prompt used for baseline experiment (thought generating) for all tasks was as follows:

\begin{verbatim}
You are an expert Python developer specialized in generating layout designs 
in GDS (GDSII) format. Your task is to assist the user in creating Python 
code that accurately draws layout designs while being mindful of the 
geometric relationships and layout accuracy.

Write down your thinking step by step before you start coding:
1. Always start by understanding the overall design requirements provided 
   by the user.
2. Break down the design into smaller components and define each geometric 
   shape with precise coordinates.
3. Ensure that all shapes and elements maintain their correct geometric 
   relationships, such as alignment, spacing, and proportional dimensions.
4. Validate each step of the design process to avoid errors and maintain 
   accuracy.

Use the 'gdspy' library to generate the GDS layout:
1. Parse the user's design specifications.
2. Define the library and cell for the GDS layout.
3. Create each geometric element (e.g., rectangles, polygons) with precise 
   coordinates.
4. Ensure elements are placed correctly and maintain their intended 
   relationships.
5. Save the design to a GDS file.

Provide all the code in a single ```python ``` block to the user without 
postamble. Do not include any other ``` block in your response to avoid 
parsing error in following steps.

Be meticulous in your approach, and always consider the geometric 
relationships and layout accuracy in every step of the design process.
\end{verbatim}

From table 1 to 5, we show selected examples from each category of tasks where the LLMs having trouble with the task.

% \input{plot_experiment.tex}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Category} & \textbf{Shapes} \\
\hline
Basic Shapes 1 & Circle, Donut, Oval, Square, Triangle, Grid \\
\hline
Basic Shapes 2 & Hexagon, Pentagon, Heptagon, Octagon, Trapezoid, Text \\
\hline
Advanced Shapes & Arrow, SquareArray, Serpentine, RoundedSquare, BasicLayout, Spiral \\
\hline
Complex Structures & MicrofluidicChip, ViaConnection, FiducialCircle, ComplexLayout, DLDChip, FinFET, RectangleWithText \\
\hline
\end{tabular}
\caption{Complete sets of shapes categorized by complexity}
\label{tab:shape_categories}
\end{table}

This comprehensive presentation of tasks, prompts, ground truths, and LLM outputs provides a clear view of the performance and capabilities of different models in generating GDSII layouts. It allows for easy comparison between the expected results and the actual outputs from various LLMs and the SOLOMON system.

\subsection{Errors in Baseline Experiment}
\label{appendix:baseline_errors}

\subsubsection{Scaling Errors}
\label{appendix:scaling_errors}

The default unit in the gdspy library is micrometers. We requested basic shapes to be drawn in millimeters to test whether LLMs could correctly handle this unit conversion. All LLMs struggled to various degrees:

(a) Some LLMs failed to pay attention to the requested unit (millimeters) and did not perform the necessary scaling.

(b) In some cases, LLMs paid attention to the requested unit but made incorrect assumptions about gdspy's default unit. We observed biased hallucinations: Llama models tended to assume millimeters, Claude models sometimes defaulted to nanometers, while GPT-4o occasionally interpreted the default unit as meters.

(c) As mentioned in the main text, Llama-3 models were especially vulnerable to this issue. They sometimes assumed the user had made a mistake by requesting millimeters, and proceeded to draw in micrometers instead, justifying this choice with comments like "not mm, as the GDSII format is in micrometers". This type of "arrogant" behavior and misalignment with human instructions on simple tasks could be harmful for deploying LLMs as fully autonomous AI agents. A recent Nature paper \cite{ZhouNature2024} has also discussed similar observations.

\subsubsection{Shape Errors}
\label{appendix:shape_errors}

Incorrect shapes often resulted from LLMs making basic arithmetic errors. For instance, in the "Hexagon" task, Llama-3.1-405B once used an internal angle of 120 degrees, producing a triangle instead of a hexagon. However, in other runs, it correctly calculated the angle based on the number of edges. Many of these errors can be mitigated through Chain-of-Thought (CoT) prompting, which encourages the model to do calculations step-by-step.

\subsubsection{Runtime Errors}
\label{appendix:runtime_errors}

This section provides a detailed breakdown of the errors encountered during the baseline experiment for each LLM. The most frequent error across all models was \textit{AttributeError: module 'gdspy' has no attribute 'LayoutViewer'}, occurring 26 times (59.09\%) with GPT-4o and 33 times (61.11\%) with Claude-3.5-Sonnet. This error was less common in other models, appearing only once each for Llama-3.1-70B and o1-preview, and not at all for Llama-3.1-405B.

The prevalence of this error indicates that GPT-4o and Claude-3.5-Sonnet attempted to provide GUI output, which was unavailable in the runtime environment. However, this issue stems from a lack of specification about the runtime environment in the prompt, rather than being entirely the LLMs' fault.

To ensure a fair comparison, we re-ran all generated code with `LayoutViewer` lines commented out. The analysis presented in Figure \ref{fig:baseline-llm-performance} and the following breakdown reflect these adjusted results.

Other common errors included hallucinations of nonexistent `gdspy` functions or methods, resulting in various `AttributeErrors` (e.g., `'CrossSection'`, `'Circular'`, `'Ellipse'`) and `TypeErrors`. Some errors were due to spelling mistakes, such as misspelling \textit{gdspy.Text} as \textit{gdspy.text}.

The subsequent analysis presents a detailed error breakdown for each LLM, ranked by ascending number of errors.

\paragraph{o1-preview}
Total errors: 12

The main errors for o1-preview included:
\begin{itemize}
    \item TypeError: GdsLibrary.write\_gds() got an unexpected keyword argument 'unit' (16.67\%)
    \item SyntaxError: invalid syntax (16.67\%)
    \item Various TypeErrors and AttributeErrors related to unexpected keyword arguments or missing attributes (66.66\%)
\end{itemize}

\paragraph{GPT-4o}
Total errors: 18

The most common errors for GPT-4o were:
\begin{itemize}
    \item TypeError related to unexpected keyword arguments (27.78\%)
    \item SyntaxError: invalid syntax (16.67\%)
    \item TypeError: 'float' object cannot be interpreted as an integer (11.11\%)
\end{itemize}

Other errors included various AttributeErrors, IndexErrors, and ValueErrors, each occurring once or twice.

\paragraph{Claude-3-5-sonnet}
Total errors: 21

The most frequent error for Claude-3-5-sonnet was:
\begin{itemize}
    \item TypeError: Text.\_\_init\_\_() got an unexpected keyword argument 'anchor' (38.10\%)
\end{itemize}

Other errors included:
\begin{itemize}
    \item TypeError: Path.\_\_init\_\_() got an unexpected keyword argument 'layer' (9.52\%)
    \item Various TypeErrors, ValueErrors, and AttributeErrors, each occurring once (52.38\%)
\end{itemize}

\paragraph{Llama-3-405b}
Total errors: 36

The most frequent errors for Llama-3-405b were:
\begin{itemize}
    \item TypeError: 'int' object is not subscriptable (8.33\%)
    \item SyntaxError: invalid syntax (8.33\%)
    \item Various TypeErrors related to unexpected keyword arguments or multiple values for arguments (16.68\%)
\end{itemize}

This model also encountered several ValueErrors and AttributeErrors.

\paragraph{Llama-3-70b}
Total errors: 68

The most prevalent error for Llama-3-70b was:
\begin{itemize}
    \item AttributeError: module 'gdspy' has no attribute 'Library'. Did you mean: 'library'? (36.76\%)
\end{itemize}

Other common errors included:
\begin{itemize}
    \item TypeError: GdsLibrary.write\_gds() got an unexpected keyword argument 'unit' (7.35\%)
    \item SyntaxError related to assignment (5.88\%)
    \item Various TypeErrors and AttributeErrors related to unexpected keyword arguments or missing attributes (25.00\%)
\end{itemize}

These error patterns suggest that all models struggled with correctly using the gdspy library, often attempting to use non-existent attributes or passing incorrect arguments to functions. Syntax errors were also common across models, indicating issues with code structure and Python syntax.

\subsubsection{Inefficient Code Example}
\label{appendix:inefficient_code}

In the DLDChip task, which involves creating a dense array of identical shapes, the Llama-3.1-405B model generated code that created a large number of objects and performed numerous boolean operations. This led to high memory usage and extended execution time, requiring the code to be terminated after approximately 15 minutes of runtime.

\subsubsection{Ambiguous Instructions}
\label{appendix:ambiguous_instructions}

In some cases, we observed that the LLM results mainly fell into two categories. After inspecting the prompts, we found that the instructions could be interpreted in two ways. In these cases, we counted both types of results as correct. However, when implementing a copilot, the agent should ask for clarification if the instructions are ambiguous.

\subsection{Via Connection Test Cases}
\label{appendix:via_connection}

Figure \ref{fig:sketch} shows the sketch used in all via connection test cases discussed in Section \ref{sec:multimodal_inputs}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{sketch.png}
\caption{Sketch used for via connection test cases}
\label{fig:sketch}
\end{figure}

\subsubsection{Prompts for Via Connection Tests}
\label{appendix:via_prompts}

\textbf{Test 1:} "I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. We want to use a metal to connect two vias and put a pad on top of each via"

\textbf{Test 2:} "I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. We want to have two vias near each end on a piece of metal. And a pad on top of the metal."

\textbf{Test 3:} "I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. we want use to connect two vias using a piece of metal and put a circular padding on top of each via"

Figure \ref{fig:test3_chat} illustrates the iterative prompting process for Test 3, as discussed in Section \ref{sec:multimodal_inputs}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\linewidth]{Styles/Test3.png}
\caption{Test 3 - Iterations to guide the model to generate desired output}
\label{fig:test3_chat}
\end{figure}

\textbf{Test 4 (Generated by LLM based on the final output in Test 3):}\\
"Layers and Colors:
The design consists of three layers: via layer (yellow), metal layer (blue), and pad layer (red).\\
Dimensions:
Via: The radius of each via is 10 units.
Pad: The radius of each pad is 30 units.
Metal Connection: The width of the metal connection is 40 units, and the total length is 600 units.\\
Positions:
The first via is positioned at coordinates (50, 150).
The second via is positioned at coordinates (550, 150).\\
Connections and Coverage:
The metal connection should fully cover the vias, extending slightly beyond their edges.
Ensure the metal connection is slightly wider than the via diameter to provide full coverage.\\
Spacing and Margins:
Leave a margin of 10 units between the edge of the metal and the pads.
Ensure there is a space of 50 units between the vias and the edges of the metal connection.\\
Additional Requirements:
The metal connection should be shorter than the total length to fit beneath the covering area of the pads, leaving some space at the edges.
By providing detailed information like this, you can ensure that the design is accurately reproduced. If you have any specific design rules or preferences, make sure to include those as well."

\textbf{Test 5:} "I have sketched a design for 3d packaging, where we have a metal connecting two TSVs , please generate the python code to draw a GDSII for this design."

\textbf{Test 6:} "I have sketched a design for 3d packaging, where we have a metal connecting two TSVs , please generate the python code to draw a GDSII based on the sketch. each color represents an individual layer. The metal connection should fully cover the vias, extending slightly beyond their edges. Ensure the metal connection is slightly wider than the via diameter to provide full coverage."

\end{document}