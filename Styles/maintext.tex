\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage{graphicx} 
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{biblatex} %Imports biblatex package
\addbibresource{bibliography.bib} %Import the bibliography file

\title{Semiconductor Layout Design with Large Language Models: Opportunities and Challenges}

\author{%
  Bo Wen\\
  IBM Watson Research Center, \\
  Yorktown Heights, NY, USA \\
  \texttt{bwen@us.ibm.com} \\
  \And
  Xin Zhang\\
  IBM Watson Research Center, \\
  Yorktown Heights, NY, USA \\
  \texttt{xzhang@us.ibm.com} \\
}

\begin{document}

\maketitle

\begin{abstract}
  This paper explores the potential of large language models (LLMs) as an adaptive "layout design copilot" in semiconductor and integrated circuit (IC) design. We evaluate the baseline performance of state-of-the-art LLMs on a set of 25 layout design tasks, revealing common mistakes and limitations. We explore spatial reasoning and knowledge application capabilities of LLMs through the Via Connection test case, demonstrating the challenges of adapting general-purpose models to specialized domains. We introduce a novel Neuro-inspired LLM Reasoning Network architecture, called SOLOMON, which significantly improves performance on these tasks through In-Context Learning and Prompt Tuning. The paper discusses the challenges of using LLMs in layout design and proposes future research directions for developing more adaptive and domain-specific foundation models.
\end{abstract}

\section{Introduction}
The semiconductor industry faces significant challenges in automating the layout design for integrated circuits (ICs), microfluidic chips, and other electronic devices. Current graphical user interface (GUI) based design workflows lack adaptability, leading to inefficient, repetitive tasks when creating similar structures with minor variations.\cite{Greengard2024-hx} This approach violates the ``Don't Repeat Yourself'' (DRY) principle and reduces productivity.

While traditional layout design software has struggled to address these challenges, Large Language Models (LLMs) offer a promising solution. LLMs' ability to understand natural language instructions and generate code makes them ideal candidates for a ``layout design copilot''. This new paradigm could allow human designers to focus on high-level tasks requiring creativity and intuition, while the LLM-powered copilot generates or modifies the layout code based on human intent.

However, adapting general-purpose LLMs to specialized domains like semiconductor layout design presents unique challenges. Both LLMs and domain-specific knowledge are rapidly evolving, making traditional fine-tuning approaches less efficient. Fine-tuning is costly, time-consuming, and requires frequent updates as underlying models change, leading to a constant need for retraining.

This paper explores the capabilities and limitations of LLMs in semiconductor layout design tasks. We evaluate the baseline performance of state-of-the-art LLMs and document an iterative design process with multimodal inputs. We then introduce a novel Neuro-inspired LLM Reasoning Network architecture called SOLOMON. This architecture doesn't rely on particular LLMs, allowing for easy replacement of underlying models while retaining learned domain knowledge through Retrieval-Augmented Generation (RAG) and prompt tuning. Our approach improves reasoning capabilities, mitigates hallucinations, and offers a more flexible solution for adapting to specialized domains. This work contributes to the broader discussion on creating adaptive foundation models that can efficiently incorporate domain-specific knowledge and personalization without the need for constant fine-tuning.

\section{Baseline LLM Performance in Layout Design}

We evaluated the performance of five large language models (LLMs) - GPT-4o\cite{GPT-4o}, Claude-3.5-Sonnet\cite{Claude-3.5-Sonnet}, Llama-3.1-70B\cite{Llama-3.1-70B}, Llama-3.1-405B\cite{Llama-3.1-405B} and the new ``reasoning model'', o1-preview\cite{o1-preview} - on a set of 25 layout design tasks. Each tasks have been run 5 times with same prompt and temperature = 0.7. The LLMs are asked to write a python code with gdspy\cite{gdspy} and generate a GDSII file. We then run the LLM written code in a Linux VM and further convert the generated GDSII file to png images for human evaluation. These tasks were categorized into four groups: Basic Shapes 1, Basic Shapes 2, Advanced Shapes, and Complex Structures. Figure \ref{fig:baseline-llm-performance} presents a summary of the LLMs' performance across the task categories. See Appendix for the task prompts and detailed results.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{baseline-llm-performance.png}
  \caption{Baseline LLM Performance on Layout Design Tasks}
  \label{fig:baseline-llm-performance}
\end{figure}

The results reveal several common mistakes and limitations of standalone LLMs in handling layout design tasks:

1. \textbf{Scaling errors}: LLMs often struggled with unit conversion, failing to scale shapes from millimeters to micrometers (gdspy's default unit). This issue was particularly prevalent in Llama-3 models as can be seen from the dark blue ``Wrong Scale'' part of bar chart. They sometimes even assumed the user had made a mistake by requesting millimeters, and proceeded to 
draw in micrometers instead, justifying this choice 
with comments like "not mm, as the GDSII format is 
micrometers". This type of ``arrogant'' behavior and 
misalignment with human instructions on simple tasks 
will be very harmful for deploying LLMs as fully 
automanous AI agents. A recent Nature paper \cite
{ZhouNature2024} has also discussed similar 
observations. (see Appendix \ref{appendix:scaling_errors} for detailed examples).

2. \textbf{Partial correctness}: In complex tasks, LLMs sometimes generated partially correct results with right shapes but wrong relative positions.

3. \textbf{Shape errors}: Incorrect shapes often resulted from basic arithmetic errors, such as miscalculating angles for polygons (see Appendix \ref{appendix:shape_errors} for specific examples). Many of these errors can be mitigated 
through Chain-of-Thought (CoT) prompting, which 
encourages the model to do calculations step-by-step.

4. \textbf{Runtime errors}: Approximately one-third of the generated code failed to execute due to various issues, including:
   - Attempts to use unavailable GUI features (particularly in GPT-4o and Claude-3.5-Sonnet)
   - Hallucinations of nonexistent gdspy functions
   - Typical programming logic mistakes
   (See Appendix \ref{appendix:runtime_errors} for a detailed error report)

5. \textbf{Inefficient code}: In one particular case, the Llama-3.1-405B model generated code with excessive object creation and boolean operations, leading to high memory usage and extended execution times (see Appendix \ref{appendix:inefficient_code} for the DLDChip task example).

6. \textbf{Ambiguous instructions}: When prompts could be interpreted in multiple ways, LLMs sometimes produced divergent results, highlighting the need for clear and specific instructions (see Appendix \ref{appendix:ambiguous_instructions} for examples).

Many of these issues can be mitigated by augmenting the LLMs with more advanced setup. Specifically, using Retrieval-Augmented Generation (RAG) to provide gdspy documentation to the LLMs, we should be able to reduce the errors related to misspelling and hallucinating nonexistent functions. For complex tasks, we can provide example code of simple shapes for In-Context Learning, so the LLMs only need to reason about positioning and scaling, and avoid runtime errors. We will discuss more in Section 3.

\section{Multimodal Inputs and Spatial Reasoning}
Layout design in semiconductor processes requires not only generating correct basic geometric shapes but also spatial reasoning to create proper ``layouts'' that meet specific requirements. Via connections, which create electrical pathways between different chip layers, exemplify this challenge. While seemingly simple—typically consisting of circular vias and rectangular metal connections—they demand precise positioning and sizing to ensure no short or open circuits and other functionality issues.

We conducted a series of tests by providing a sketch(image) together with different text prompts. The sketch are color-coded to represent different layers (e.g., yellow for via, blue for metal, red for pad) to help LLM understand the spatial relationships. We varied the text prompts to test the LLMs' spatial reasoning. We used GPT-4o's vision input capability for this experiment. Figure \ref{fig:via_experiment} illustrates the sketch inputs and corresponding LLM-generated outputs for each test case.
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{Figure1_v5.png}
\caption{Sketch input and LLM-generated outputs for the via connection experiment. The sketch depicts a desired layout with two vias connected by a metal layer and circular pads on top. The outputs show the progression of the LLM's understanding and refinement of the layout based on iterative feedback and context provided by the user.}
\label{fig:via_experiment}
\end{figure}
In \textbf{Test 1}, we provided a basic sketch (Figure ~\ref{fig:via_experiment}(a)) with simple instructions. The LLM generated code, but the output had issues, including insufficient metal connection width. \textbf{Test 2} involved refining the description, but the output incorrectly used square vias and pads, and failed to properly cover the vias with metal. In \textbf{Test 3}, we provided a more detailed description. After seven iterations of feedback, the LLM finally produced the correct layout (Figure ~\ref{fig:via_experiment}(d)). See Appendix \ref{appendix:via_connection} for detailed prompts and the iterative process.

\textbf{Test 4} reversed the process by asking the LLM to create a detailed prompt based on the correct layout from \textbf{Test 3} (Appendix \ref{appendix:via_connection}). The LLM described each component's size and location in detail but hallucinated an additional requirement: \textit{a 50-unit space between the vias and the edges of the metal connection}. This would result in the layout shown in the dotted rectangle in Figure~\ref{fig:via_experiment}(e), where the metal extends beyond the contact pad. Interestingly, when given this ``wrong'' prompt, GPT-4o ignored the added specification and produced a layout matching the original design, with the metal not extending beyond the pad. Code inspection revealed that the LLM used another requirement, \textit{Leave a margin of 10 units between the edge of the metal and the pads}, to calculate the metal edge position in both x and y directions, although this was intended only for the y-direction margin. Using this version of prompt in the baseline evaluations (Section 1), o1-preview and Llama-3.1-405B each produced the ``non-extending'' version in one out of 5 runs, indicating some ambiguity in the specification. Generally, providing specific numerical values for size and location of geometric shapes in the prompt proves more robust than simple instructions like ``draw me a via''.

To further test our hypothesis, we conducted \textbf{Tests 5} and \textbf{6}, removing numerical values from the prompt and incorporating domain-specific context (e.g., 3D packaging and Through-Silicon Vias). This approach, however, degraded LLM performance, revealing a critical limitation: while LLMs possess textbook knowledge of semiconductor concepts, they struggle to translate this into practical design requirements. For instance, LLMs failed to apply common engineering knowledge, such as using wider metal layers to connect vias or leaving margin space between components in different layers to account for layermisalignment.

This finding underscores the importance of improving LLMs' reasoning capability of applying domain knowledge in problem-solving, rather than simply increasing model size to remember more knowledge, to enhance the adaptability of LLM-based AI systems.

\section{Neuro-inspired LLM Reasoning Network Architecture}

Based on our observations, we hypothesize that LLMs' poor performance in layout design tasks stems from weak spatial reasoning and difficulty applying domain knowledge.

To test this hypothesis, we employed the IBM proprietary System for Optimizing Language Outputs through Multi-agent Oversight Networks (SOLOMON). Originally developed as part of a grant proposal for the ARPA-H ADVANCED RESEARCH PROJECTS AGENCY FOR HEALTH CHATBOT ACCURACY AND RELIABILITY EVALUATION (CARE) EXPLORATION TOPIC, SOLOMON was designed to detect and correct LLM hallucinations in healthcare applications. The system is based on a Neuro-inspired LLM Reasoning Network Architecture, drawing inspiration from Brain-like AGI \cite{Brain-AGI} and the Free Energy Principle (FEP) theory \cite{FEP}.

At its core, SOLOMON's key innovation lies in its multi-layered approach: 
(1) Thought Generators: A pool of multiple LLMs, each with distinct knowledge bases and reasoning abilities, trained on diverse datasets and fine-tuned for specific instruction-following capabilities.
(2) Thought Assessor: An LLM-based system that leverages the Free Energy Principle to analyze the consensus and differences among the proposed "Thoughts" from the Thought Generators.
(3) Steering Subsystem: Currently human-operated, this component provides instructions to guide the Thought Assessor in generating the final output.

This architecture enables SOLOMON to combine diverse perspectives, assess their validity, and produce a refined output, potentially mitigating individual LLM limitations in spatial reasoning and domain knowledge application.
\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{judges-performance.png}
  \caption{Performance comparison between 4 instances of SOLOMON built with classic non-reasoning LLMs versus o1-preview, the SOTA of reasoning LLM.}
  \label{fig:judges-performance}
\end{figure}
Figure \ref{fig:judges-performance} shows SOLOMON's performance on the same 25 tasks. We used 20 "Thoughts" from GPT-4o, Claude, and two Llama-3.1 models, including code, error reports, and rendered GDSII images (for GPT-4o and Claude only). We created 4 SOLOMON instances, each using one of the 4 LLMs as a Thought Assessor. The o1-preview result is shown side-by-side for comparison purposes only, its output is not used in SOLOMON's pool of thoughts.

Performance improved for all 4 LLMs compared to their baseline, with runtime errors almost completely eliminated. Improvement was more significant for GPT-4o and Claude due to their vision input capability, which assists spatial reasoning. Without vision input, Llama-3.1 models could only avoid runtime errors but struggled to evaluate spatial layouts effectively. We can see that SOLOMON's performance is comparable to o1-preview, the SOTA of reasoning LLM.

\section{Conclusion and Future Work}
The introduction of the SOLOMON architecture significantly improved performance, particularly in reducing runtime errors and enhancing spatial reasoning capabilities. However, challenges remain in translating domain knowledge into practical design requirements.

Future research directions include:
(1) Creating comprehensive benchmarks for LLM capabilities in layout design tasks.
(2) Exploring more configurations of SOLOMON architecture to understand the interaction between quality of initial thoughts vs assessment by ensemble, i.e., system 1 vs system 2 contributions in reasoning tasks.
(3) Exploring stacking layers of SOLOMON architecture to form a hierarchical reasoning model, for recalling and reasoning domain knowledge for solving the task on hand.
(4) Exploring the adaptability of SOLOMON architecture across different domains.

While LLMs show promise as layout design copilots, further advancements in reasoning capabilities and domain knowledge application are necessary for their effective integration into semiconductor design processes and other domain specific tasks.

\textbf{Acknowledgment:} We thank Kuan Yu Hsieh for her valuable contribution in creating the dataset of 25 tasks with ground truth and her exploration work on the via connection test cases.

\printbibliography %Prints bibliography

\newpage
\appendix

\section{Appendix}
\subsection{Task Prompts and Baseline LLM Performance}
\label{appendix:task_prompts_and_performance}

This section presents the 25 tasks used in our evaluation, along with the performance of various LLMs and the SOLOMON system. For each task, we provide the prompt, the ground truth image, and a table showing the outputs from different LLMs across 5 runs, as well as the SOLOMON result.

The system prompt used for baseline experiment (thought generating) for all tasks was as follows:

\begin{verbatim}
You are an expert Python developer specialized in generating layout designs 
in GDS (GDSII) format. Your task is to assist the user in creating Python 
code that accurately draws layout designs while being mindful of the 
geometric relationships and layout accuracy.

Write down your thinking step by step before you start coding:
1. Always start by understanding the overall design requirements provided 
   by the user.
2. Break down the design into smaller components and define each geometric 
   shape with precise coordinates.
3. Ensure that all shapes and elements maintain their correct geometric 
   relationships, such as alignment, spacing, and proportional dimensions.
4. Validate each step of the design process to avoid errors and maintain 
   accuracy.

Use the 'gdspy' library to generate the GDS layout:
1. Parse the user's design specifications.
2. Define the library and cell for the GDS layout.
3. Create each geometric element (e.g., rectangles, polygons) with precise 
   coordinates.
4. Ensure elements are placed correctly and maintain their intended 
   relationships.
5. Save the design to a GDS file.

Provide all the code in a single ```python ``` block to the user without 
postamble. Do not include any other ``` block in your response to avoid 
parsing error in following steps.

Be meticulous in your approach, and always consider the geometric 
relationships and layout accuracy in every step of the design process.
\end{verbatim}

From table 1 to 5, we show selected examples from each category of tasks where the LLMs having trouble with the task.

\input{plot_experiment.tex}

This comprehensive presentation of tasks, prompts, ground truths, and LLM outputs provides a clear view of the performance and capabilities of different models in generating GDSII layouts. It allows for easy comparison between the expected results and the actual outputs from various LLMs and the SOLOMON system.

\subsection{Errors in Baseline Experiment}
\label{appendix:baseline_errors}

\subsubsection{Scaling Errors}
\label{appendix:scaling_errors}

The default unit in the gdspy library is micrometers. We requested basic shapes to be drawn in millimeters to test whether LLMs could correctly handle this unit conversion. All LLMs struggled to various degrees:

(a) Some LLMs failed to pay attention to the requested unit (millimeters) and did not perform the necessary scaling.

(b) In some cases, LLMs paid attention to the requested unit but made incorrect assumptions about gdspy's default unit. We observed hallucinations of millimeters (Llama), nanometers (Claude), and meters (GPT-4o).

(c) As mentioned in the main text, Llama-3 models were especially vulnerable to this issue. They sometimes assumed the user had made a mistake by requesting millimeters, and proceeded to draw in micrometers instead, justifying this choice with comments like "not mm, as the GDSII format is in micrometers". This type of "arrogant" behavior and misalignment with human instructions on simple tasks could be harmful for deploying LLMs as fully autonomous AI agents. A recent Nature paper \cite{ZhouNature2024} has also discussed similar observations.

\subsubsection{Shape Errors}
\label{appendix:shape_errors}

Incorrect shapes often resulted from LLMs making basic arithmetic errors. For instance, in the "Hexagon" task, Llama-3.1-405B once used an internal angle of 120 degrees, producing a triangle instead of a hexagon. However, in other runs, it correctly calculated the angle based on the number of edges. Many of these errors can be mitigated through Chain-of-Thought (CoT) prompting, which encourages the model to do calculations step-by-step.

\subsubsection{Runtime Errors}
\label{appendix:runtime_errors}

This section provides a detailed breakdown of the errors encountered during the baseline experiment for each LLM. The most frequent error across all models was \textit{AttributeError: module 'gdspy' has no attribute 'LayoutViewer'}, occurring 26 times (59.09\%) with GPT-4o and 33 times (61.11\%) with Claude-3.5-Sonnet. This error was less common in other models, appearing only once each for Llama-3.1-70B and o1-preview, and not at all for Llama-3.1-405B.

The prevalence of this error indicates that GPT-4o and Claude-3.5-Sonnet attempted to provide GUI output, which was unavailable in the runtime environment. However, this issue stems from a lack of specification about the runtime environment in the prompt, rather than being entirely the LLMs' fault.

To ensure a fair comparison, we re-ran all generated code with `LayoutViewer` lines commented out. The analysis presented in Figure \ref{fig:baseline-llm-performance} and the following breakdown reflect these adjusted results.

Other common errors included hallucinations of nonexistent `gdspy` functions or methods, resulting in various `AttributeErrors` (e.g., `'CrossSection'`, `'Circular'`, `'Ellipse'`) and `TypeErrors`. Some errors were due to spelling mistakes, such as misspelling \textit{gdspy.Text} as \textit{gdspy.text}.

The subsequent analysis presents a detailed error breakdown for each LLM, ranked by ascending number of errors.

\paragraph{o1-preview}
Total errors: 12

The main errors for o1-preview included:
\begin{itemize}
    \item TypeError: GdsLibrary.write\_gds() got an unexpected keyword argument 'unit' (16.67\%)
    \item SyntaxError: invalid syntax (16.67\%)
    \item Various TypeErrors and AttributeErrors related to unexpected keyword arguments or missing attributes (66.66\%)
\end{itemize}

\paragraph{GPT-4o}
Total errors: 18

The most common errors for GPT-4o were:
\begin{itemize}
    \item TypeError related to unexpected keyword arguments (27.78\%)
    \item SyntaxError: invalid syntax (16.67\%)
    \item TypeError: 'float' object cannot be interpreted as an integer (11.11\%)
\end{itemize}

Other errors included various AttributeErrors, IndexErrors, and ValueErrors, each occurring once or twice.

\paragraph{Claude-3-5-sonnet}
Total errors: 21

The most frequent error for Claude-3-5-sonnet was:
\begin{itemize}
    \item TypeError: Text.\_\_init\_\_() got an unexpected keyword argument 'anchor' (38.10\%)
\end{itemize}

Other errors included:
\begin{itemize}
    \item TypeError: Path.\_\_init\_\_() got an unexpected keyword argument 'layer' (9.52\%)
    \item Various TypeErrors, ValueErrors, and AttributeErrors, each occurring once (52.38\%)
\end{itemize}

\paragraph{Llama-3-405b}
Total errors: 36

The most frequent errors for Llama-3-405b were:
\begin{itemize}
    \item TypeError: 'int' object is not subscriptable (8.33\%)
    \item SyntaxError: invalid syntax (8.33\%)
    \item Various TypeErrors related to unexpected keyword arguments or multiple values for arguments (16.68\%)
\end{itemize}

This model also encountered several ValueErrors and AttributeErrors.

\paragraph{Llama-3-70b}
Total errors: 68

The most prevalent error for Llama-3-70b was:
\begin{itemize}
    \item AttributeError: module 'gdspy' has no attribute 'Library'. Did you mean: 'library'? (36.76\%)
\end{itemize}

Other common errors included:
\begin{itemize}
    \item TypeError: GdsLibrary.write\_gds() got an unexpected keyword argument 'unit' (7.35\%)
    \item SyntaxError related to assignment (5.88\%)
    \item Various TypeErrors and AttributeErrors related to unexpected keyword arguments or missing attributes (25.00\%)
\end{itemize}

These error patterns suggest that all models struggled with correctly using the gdspy library, often attempting to use non-existent attributes or passing incorrect arguments to functions. Syntax errors were also common across models, indicating issues with code structure and Python syntax.

\subsubsection{Inefficient Code Example}
\label{appendix:inefficient_code}

In the DLDChip task, which involves creating a dense array of identical shapes, the Llama-3.1-405B model generated code that created a large number of objects and performed numerous boolean operations. This led to high memory usage and extended execution time, requiring the code to be terminated after approximately 15 minutes of runtime.

\subsubsection{Ambiguous Instructions}
\label{appendix:ambiguous_instructions}

In some cases, we observed that the LLM results mainly fell into two categories. After inspecting the prompts, we found that the instructions could be interpreted in two ways. In these cases, we counted both types of results as correct. However, when implementing a copilot, the agent should ask for clarification if the instructions are ambiguous.

\subsection{Via Connection Test Cases}
\label{appendix:via_connection}

Figure \ref{fig:sketch} shows the sketch used in all via connection test cases discussed in Section \ref{sec:multimodal_inputs}.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{sketch.png}
\caption{Sketch used for via connection test cases}
\label{fig:sketch}
\end{figure}

\subsubsection{Prompts for Via Connection Tests}
\label{appendix:via_prompts}

\textbf{Test 1:} "I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. We want to use a metal to connect two vias and put a pad on top of each via"

\textbf{Test 2:} "I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. We want to have two vias near each end on a piece of metal. And a pad on top of the metal."

\textbf{Test 3:} "I have a sketch idea that i want to draw in GDSII, generate the python code for this design. each color represents an individual layer. we want use to connect two vias using a piece of metal and put a circular padding on top of each via"

Figure \ref{fig:test3_chat} illustrates the iterative prompting process for Test 3, as discussed in Section \ref{sec:multimodal_inputs}.

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\linewidth]{Styles/Test3.png}
\caption{Test 3 - Iterations to guide the model to generate desired output}
\label{fig:test3_chat}
\end{figure}

\textbf{Test 4 (Generated by LLM based on the final output in Test 3):}\\
"Layers and Colors:
The design consists of three layers: via layer (yellow), metal layer (blue), and pad layer (red).\\
Dimensions:
Via: The radius of each via is 10 units.
Pad: The radius of each pad is 30 units.
Metal Connection: The width of the metal connection is 40 units, and the total length is 600 units.\\
Positions:
The first via is positioned at coordinates (50, 150).
The second via is positioned at coordinates (550, 150).\\
Connections and Coverage:
The metal connection should fully cover the vias, extending slightly beyond their edges.
Ensure the metal connection is slightly wider than the via diameter to provide full coverage.\\
Spacing and Margins:
Leave a margin of 10 units between the edge of the metal and the pads.
Ensure there is a space of 50 units between the vias and the edges of the metal connection.\\
Additional Requirements:
The metal connection should be shorter than the total length to fit beneath the covering area of the pads, leaving some space at the edges.
By providing detailed information like this, you can ensure that the design is accurately reproduced. If you have any specific design rules or preferences, make sure to include those as well."

\textbf{Test 5:} "I have sketched a design for 3d packaging, where we have a metal connecting two TSVs , please generate the python code to draw a GDSII for this design."

\textbf{Test 6:} "I have sketched a design for 3d packaging, where we have a metal connecting two TSVs , please generate the python code to draw a GDSII based on the sketch. each color represents an individual layer. The metal connection should fully cover the vias, extending slightly beyond their edges. Ensure the metal connection is slightly wider than the via diameter to provide full coverage."

\end{document}